{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Feature Engineering\n",
    "In this section of the lab we will develop and run a Python script that pre-processes training and validation images into a set of powerful features - refered to in the lab as bottleneck features.\n",
    "\n",
    "To create bottleneck features we will utilize a pre-trained Deep Learning network that was trained on a general computer vision domain. \n",
    "\n",
    "As explained by your instructor this approach is called Transfer Learning. Transfer Learning is a powerful Machine Learning technique that is based on an observation that the knowledge gained while solving one problem can be applied to a different (but related problem).\n",
    "\n",
    "In the context of an image classification task, a DNN trained on one visual domain can accelerate learing in another visual domain. Although, our pre-trained network does not know how to classify aerial land plot images, it knows enough about representing image concepts that if we use it to pre-process aerial images, the extracted image features can be used to effectively train a relatively simple classifier on a **limited number** of samples.\n",
    "\n",
    "The below diagram represents the architecture of our solution.\n",
    "\n",
    "![Transfer Learning](../images/TLArch.png)\n",
    "\n",
    "We will use **ResNet50** trained on **imagenet** dataset to extract features. We will occasionally refer to this component of the solution as a featurizer. The output of the featurizer is a vector of 2048 floating point numbers, each representing a feature extracted from an image. \n",
    "\n",
    "We will then use extracted features to train a simple fully connected neural network (FCNN) to classify aerial land plot images. We could use any other classification algorithm - e.g. logistic regression or decision trees but FCNN gives us a lot flexibility in fine tuning the model.\n",
    "\n",
    "\n",
    "The Python script generated by the next Jupyter cell processes an input image dataset into an output bottleneck feature set. The script expects the images to be organized in the below folder structure:\n",
    "```\n",
    "train/\n",
    "   Barren/\n",
    "   Cultivated/\n",
    "   Developed/\n",
    "   Forest/\n",
    "   Herbaceous/\n",
    "   Shrub/\n",
    "valid/\n",
    "   Barren/\n",
    "   Cultivated/\n",
    "   Developed/\n",
    "   Forest/\n",
    "   Herbaceous/\n",
    "   Shrub/\n",
    "```\n",
    "\n",
    "The location of the input dataset and the location where to save the output dataset are passed to the script as command line parameters. The output dataset will be stored in a binary HDF5 data format used commonly in Machine Learning and High Performance Computing solutions.\n",
    "\n",
    "The script is designed to work with a large number of images. As such it does not load all input images to memory at once. Instead it utilizes custom Python generator class - `ImageGenerator` to feed the **ResNet50** featurizer. The class yields batches of images - as Numpy arrays - preprocessed to the format required by **ResNet50**. \n",
    "\n",
    "We will not attempt to run the script in a local environment. It is very computationally intensive and unless you run it in an evironment equipped with a powerful GPU it would be very slow. It would be *painfully* slow if you attempt to run it in Azure Notebooks.\n",
    "\n",
    "Instead we will run the script on a remote Azure GPU VM.\n",
    "\n",
    "Your instructor will dive into the code in the script and explain key snippets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data pre-processing script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a folder to hold the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder = './script'\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Jupyter `%%writefile` magic to write the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting script/extract.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/extract.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import resnet50\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# This is a generator that yields batches of preprocessed images\n",
    "class ImageGenerator(tf.keras.utils.Sequence):    \n",
    "    \n",
    "    def __init__(self, img_dir, preprocess_fn=None, batch_size=64):\n",
    "        \n",
    "        # Create the dictionary that maps class names into numeric labels \n",
    "        folders = os.listdir(img_dir)\n",
    "        folders.sort()\n",
    "        indexes = range(len(folders))\n",
    "        label_map = {key: value for (key, value) in zip(folders, indexes)}\n",
    "        self.num_classes = len(label_map)\n",
    "        \n",
    "        # Create a list of all images in a root folder with associated numeric labels\n",
    "        labeled_image_list = [(os.path.join(img_dir, folder, image), label_map[folder]) \n",
    "                              for folder in folders \n",
    "                              for image in os.listdir(os.path.join(img_dir, folder))\n",
    "                              ]\n",
    "        # Shuffle the list\n",
    "        random.shuffle(labeled_image_list)\n",
    "        # Set image list and associated label list\n",
    "        self.image_list, self.label_list = zip(*labeled_image_list) \n",
    "        # Set batch size\n",
    "        self.batch_size = batch_size\n",
    "       \n",
    "        # Set the pre-processing function passed as a parameter\n",
    "        self.preprocess_fn = preprocess_fn\n",
    "        \n",
    "        # Set number of batches\n",
    "        self.n_batches = len(self.image_list) // self.batch_size\n",
    "        if len(self.image_list) % self.batch_size > 0:\n",
    "            self.n_batches += 1\n",
    "            \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.n_batches\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        pathnames = self.image_list[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        images = self.__load_images(pathnames)\n",
    "        \n",
    "        return images\n",
    "    \n",
    "    # Load a set of images passed as a parameter into a NumPy array\n",
    "    def __load_images(self, pathnames):\n",
    "        images = []\n",
    "        for pathname in pathnames:\n",
    "            img = image.load_img(pathname, target_size=(224,224,3))\n",
    "            img = image.img_to_array(img)\n",
    "            images.append(img)\n",
    "        images = np.asarray(images)\n",
    "        if self.preprocess_fn != None:\n",
    "            images = self.preprocess_fn(images)   \n",
    "        \n",
    "        return images\n",
    "    \n",
    "    # Return labels in one-hot encoding\n",
    "    def get_labels(self):\n",
    "        \n",
    "        return to_categorical(np.asarray(self.label_list), self.num_classes)\n",
    "    \n",
    "\n",
    "\n",
    "def create_bottleneck_features():\n",
    "    # Configure input directories\n",
    "    train_images_dir = os.path.join(FLAGS.input_data_dir, 'train')\n",
    "    valid_images_dir = os.path.join(FLAGS.input_data_dir, 'valid')\n",
    "\n",
    "    # Create generators for training and validation data\n",
    "    #train_generator = ImageGenerator(train_images_dir, resnet50.preprocess_input)\n",
    "    #valid_generator = ImageGenerator(valid_images_dir, resnet50.preprocess_input)\n",
    "    \n",
    "    train_generator = ImageGenerator(train_images_dir, VGG16.preprocess_input)\n",
    "    valid_generator = ImageGenerator(valid_images_dir, VGG16.preprocess_input)\n",
    "\n",
    "    # Create a featurizer\n",
    "    #featurizer = resnet50.ResNet50(\n",
    "    #            weights = 'imagenet', \n",
    "    #            input_shape=(224,224,3), \n",
    "    #            include_top = False,\n",
    "    #            pooling = 'avg')\n",
    "    \n",
    "    featurizer = VGG16.vgg16(\n",
    "                weights = 'imagenet', \n",
    "                input_shape=(224,224,3), \n",
    "                include_top = False,\n",
    "                pooling = 'avg')\n",
    "    \n",
    "\n",
    "    # Generate training bottleneck features\n",
    "    print(\"Generating training bottleneck features\")\n",
    "    features = featurizer.predict_generator(train_generator, verbose=1)\n",
    "    labels = train_generator.get_labels()\n",
    "    \n",
    "    # Save training dataset to HDF5 file\n",
    "    filename = 'aerial_bottleneck_train_vgg16.h5'\n",
    "    output_file = os.path.join(FLAGS.output_data_dir, filename)\n",
    "    print(\"Saving training features to {}\".format(output_file))\n",
    "    print(\"   Training features: \", features.shape)\n",
    "    print(\"   Training labels: \", labels.shape)\n",
    "    with h5py.File(output_file, \"w\") as hfile:\n",
    "        features_dset = hfile.create_dataset('features', data=features)\n",
    "        labels_dset = hfile.create_dataset('labels', data=labels)\n",
    "\n",
    "     # Generate validation bottleneck features\n",
    "    print(\"Generating validation bottleneck features\")\n",
    "    features = featurizer.predict_generator(valid_generator, verbose=1)\n",
    "    labels = valid_generator.get_labels()\n",
    "    \n",
    "    # Save validation dataset to HDF5 file\n",
    "    filename = 'aerial_bottleneck_valid_vgg16.h5'\n",
    "    output_file = os.path.join(FLAGS.output_data_dir, filename)\n",
    "    print(\"Saving validation features to {}\".format(output_file))\n",
    "    print(\"   Validation features: \", features.shape)\n",
    "    print(\"   Validation labels: \", labels.shape)\n",
    "    with h5py.File(output_file, \"w\") as hfile:\n",
    "        features_dset = hfile.create_dataset('features', data=features)\n",
    "        labels_dset = hfile.create_dataset('labels', data=labels)\n",
    "    \n",
    "    print(\"Done\")\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "# Default global parameters\n",
    "tf.app.flags.DEFINE_integer('batch_size', 64, \"Number of images per batch\")\n",
    "tf.app.flags.DEFINE_string('input_data_dir', 'aerialsmall', \"Folder with training and validation images\")\n",
    "tf.app.flags.DEFINE_string('output_data_dir', 'bottleneck', \"A folder for saving bottleneck features\")\n",
    "\n",
    "\n",
    "def main(argv=None):\n",
    "    print(\"Starting\")\n",
    "    print(\"Reading training data from:\", FLAGS.input_data_dir)\n",
    "    print(\"Output bottleneck files will be saved to:\", FLAGS.output_data_dir)\n",
    "    os.makedirs(FLAGS.output_data_dir, exist_ok=True)\n",
    "   \n",
    "    create_bottleneck_features()\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Compute Target\n",
    "Our script is now ready for execution. We will run the script on a remote GPU equipped Azure Virtual Machine. We will use Azure ML Python SDK to create and configure the VM, datastores, and run configuration.\n",
    "\n",
    "The first step is to intialize the AML workspace you created during the lab overview."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize AML Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the config file in: /home/demouser/repos/HighPerformanceTensorFlowOnAzure/aml_config/config.json\n",
      "jkaml\n",
      "jkaml\n",
      "eastus2\n",
      "952a710c-8d9c-40c1-9fec-f752138cc0b3\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config('../aml_config/config.json')\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Azure DSVM as a compute target\n",
    "\n",
    "We will use Azure Data Science Virtual Machine equipped with Tesla K80 GPU as a compute target. If the VM is already in the workspace this code uses it and skips the creation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing DSVM: gpudsvm\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import DsvmCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "compute_target_name = 'gpudsvm'\n",
    "compute_target_type = 'Standard_NC6'\n",
    "\n",
    "try:\n",
    "    dsvm_compute = DsvmCompute(workspace=ws, name=compute_target_name)\n",
    "    print('Found existing DSVM:', dsvm_compute.name)\n",
    "except ComputeTargetException:\n",
    "    dsvm_config = DsvmCompute.provisioning_configuration(vm_size=compute_target_type)\n",
    "    dsvm_compute = DsvmCompute.create(ws, name=compute_target_name, provisioning_configuration=dsvm_config)\n",
    "    dsvm_compute.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Datastores \n",
    "The dataset we will use for training has been uploaded to a public Azure blob storage container. We will register this container as a datastore within our workspace. Before the data prep script runs, the datastore's content - training and validation images - will be copied to the local storage on DSVM.\n",
    "\n",
    "The output of the script - bottleneck files - will be pushed to the default datastore that was created automatically when you created your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registering datastore failed with 400 error code and error message\n",
      "b'{\\n  \"error\": {\\n    \"code\": \"UserError\",\\n    \"message\": \"Another data store with the same name already exists but with different values. Please use patch to update.\",\\n    \"target\": null,\\n    \"details\": [],\\n    \"innerError\": null,\\n    \"debugInfo\": {\\n      \"type\": \"Microsoft.MachineLearning.Common.WebApi.Exceptions.BadRequestException\",\\n      \"message\": \"Another data store with the same name already exists but with different values. Please use patch to update.\",\\n      \"stackTrace\": \"   at Microsoft.MachineLearning.DataStore.EntryPoints.Controllers.DataStoreController.CreateOrUpdate(DataStoreDto dto, Boolean create, Boolean createIfNotExists) in /home/vsts/work/1/s/src/azureml-api/src/DataStore/EntryPoints/Controllers/DataStoreController.cs:line 120\\\\n   at Microsoft.MachineLearning.DataStore.EntryPoints.Controllers.DataStoreController.Create(DataStoreDto dto, Boolean createIfNotExists) in /home/vsts/work/1/s/src/azureml-api/src/DataStore/EntryPoints/Controllers/DataStoreController.cs:line 59\\\\n   at Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker.InvokeActionMethodAsync()\\\\n   at Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker.InvokeNextActionFilterAsync()\\\\n   at Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker.Rethrow(ActionExecutedContext context)\\\\n   at Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker.Next(State& next, Scope& scope, Object& state, Boolean& isCompleted)\\\\n   at Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker.InvokeInnerFilterAsync()\\\\n   at Microsoft.AspNetCore.Mvc.Internal.ResourceInvoker.InvokeNextExceptionFilterAsync()\",\\n      \"innerException\": null,\\n      \"data\": {},\\n      \"errorResponse\": null\\n    }\\n  },\\n  \"correlation\": {\\n    \"operation\": \"7dcf49da-4907257fcafe71f3\",\\n    \"request\": \"Z+OzTpQfdXE=\"\\n  }\\n}'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing datastore for input images: input_images\n",
      "input_images AzureBlob azureailabs aerialmed\n",
      "Using the default datastore for output: \n",
      "workspacefilestore AzureFile jkamlstoragekhsgpljj azureml-filestore-381bfd16-8663-49a4-9e97-330161b029bd\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Datastore\n",
    "\n",
    "images_account = 'azureailabs'\n",
    "images_container = 'aerialmed'\n",
    "datastore_name = 'input_images'\n",
    "\n",
    "try:\n",
    "    input_ds = Datastore.register_azure_blob_container(workspace=ws, datastore_name=datastore_name,\n",
    "                                            container_name=images_container,\n",
    "                                            account_name=images_account)\n",
    "    print('Creating new datastore for input images')\n",
    "except:\n",
    "    input_ds = Datastore(ws, datastore_name)\n",
    "    print('Found existing datastore for input images:', input_ds.name)\n",
    "   \n",
    "print(input_ds.name, input_ds.datastore_type, input_ds.account_name, input_ds.container_name)\n",
    "\n",
    "output_ds = ws.get_default_datastore()\n",
    "print(\"Using the default datastore for output: \")\n",
    "print(output_ds.name, output_ds.datastore_type, output_ds.account_name, output_ds.container_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Experiment\n",
    "**Experiment** is a logical container in an Azure ML Workspace. It hosts run records which can include run metrics and output artifacts from your experiments. We will use **Experiment** to store logs generated by our script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'aerial-classifier-dataprep'\n",
    "\n",
    "from azureml.core import Experiment\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run\n",
    "\n",
    "We are now ready to run the script on the cluster. There are multiple ways to run the job. We are going to utilize a higher level **Estimator** object and run the script in a docker container and a remote DSVM. \n",
    "\n",
    "The call to start the run is asynchronous, it returns a **Preparing** or **Running** state as soon as the job is started.\n",
    "\n",
    "### Monitor a remote run\n",
    "\n",
    "The first run takes longer. The subsequent runs, as long as the script dependencies don't change, are much faster.\n",
    "\n",
    "Here is what's happening while you wait:\n",
    "\n",
    "- **Image creation**: A Docker image is created matching the Python environment specified by the estimator. In our case, this will be a base GPU image with the latest version of `tensorflow-gpu`, `h5py`, and `pillow`. The image is uploaded to the workspace. This stage happens once for each Python environment since the container is cached for subsequent runs.  During image creation, logs are streamed to the run history. You can monitor the image creation progress using these logs.\n",
    "\n",
    "- **Running**: In this stage, the dataprep script is sent to the DSVM, then the data in the input datastore is copied to the local storage on DSVM, then the script is run. While the job is running, stdout and the ./logs directory are streamed to the run history. You can monitor the run's progress using these logs. \n",
    "\n",
    "- **Post-Processing**: The created bottleneck files are copied to the default datastore. The ./outputs directory on the DSVM  is copied over to the run history in your workspace so you can access these results.\n",
    "\n",
    "\n",
    "You can check the progress of a running job in multiple ways. This lab uses a Jupyter widget as well as a `wait_for_completion` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "# Define the location of the dataprep script and the location for the output bottleneck files\n",
    "script_folder = 'script'\n",
    "script_name = 'extract.py'\n",
    "output_dir = './bottleneck'\n",
    "pip_packages = ['h5py','pillow','tensorflow-gpu']\n",
    "\n",
    "script_params = {\n",
    "    '--input_data_dir': input_ds.as_download(),\n",
    "    '--output_data_dir': output_dir\n",
    "}\n",
    "\n",
    "est = Estimator(source_directory=script_folder,\n",
    "                script_params=script_params,\n",
    "                compute_target=dsvm_compute,\n",
    "                entry_script=script_name,\n",
    "                node_count=1,\n",
    "                process_count_per_node=1,\n",
    "                use_gpu=True,\n",
    "                pip_packages=pip_packages,\n",
    "                inputs=[output_ds.path(output_dir).as_upload(path_on_compute=output_dir)])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>aerial-classifier-dataprep</td><td>aerial-classifier-dataprep_1540157141770</td><td>azureml.scriptrun</td><td>Running</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/952a710c-8d9c-40c1-9fec-f752138cc0b3/resourceGroups/jkaml/providers/Microsoft.MachineLearningServices/workspaces/jkaml/experiments/aerial-classifier-dataprep/runs/aerial-classifier-dataprep_1540157141770\" target=\"_blank\" rel=\"noopener\">Link to Azure Portal</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: aerial-classifier-dataprep,\n",
       "Id: aerial-classifier-dataprep_1540157141770,\n",
       "Type: azureml.scriptrun,\n",
       "Status: Running)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = {\"DNN\": \"VGG16\"}\n",
    "run = exp.submit(config=est, tags=tags)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monitor the run using Jupiter Run widget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19a1b9bb4e94d0ebb568acd777bbf47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRun(widget_settings={'childWidgetDisplay': 'popup'})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.train.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monitor the run using `wait_for_completion` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'runId': 'aerial-classifier-dataprep_1540157141770',\n",
       " 'target': 'gpudsvm',\n",
       " 'status': 'Finalizing',\n",
       " 'startTimeUtc': '2018-10-21T21:25:44.480345Z',\n",
       " 'properties': {'azureml.runsource': 'experiment',\n",
       "  'ContentSnapshotId': '1e46a00d-6dc0-4169-a196-d42d369122b1'},\n",
       " 'runDefinition': {'Script': 'extract.py',\n",
       "  'Arguments': ['--input_data_dir',\n",
       "   '$AZUREML_DATAREFERENCE_input_images',\n",
       "   '--output_data_dir',\n",
       "   './bottleneck'],\n",
       "  'Framework': 0,\n",
       "  'Target': 'gpudsvm',\n",
       "  'DataReferences': {'93ef68f2ce0a47ab9ebca75ba4b64199': {'DataStoreName': 'workspacefilestore',\n",
       "    'Mode': 'Upload',\n",
       "    'PathOnDataStore': './bottleneck',\n",
       "    'PathOnCompute': './bottleneck',\n",
       "    'Overwrite': False},\n",
       "   'input_images': {'DataStoreName': 'input_images',\n",
       "    'Mode': 'Download',\n",
       "    'PathOnDataStore': None,\n",
       "    'PathOnCompute': None,\n",
       "    'Overwrite': False}},\n",
       "  'JobName': None,\n",
       "  'AutoPrepareEnvironment': True,\n",
       "  'MaxRunDurationSeconds': None,\n",
       "  'Environment': {'Python': {'InterpreterPath': 'python',\n",
       "    'UserManagedDependencies': False,\n",
       "    'CondaDependencies': {'name': 'project_environment',\n",
       "     'dependencies': ['python=3.6.2',\n",
       "      {'pip': ['azureml-defaults', 'h5py', 'pillow', 'tensorflow-gpu']}]},\n",
       "    'CondaDependenciesFile': None},\n",
       "   'EnvironmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE',\n",
       "    'NCCL_SOCKET_IFNAME': '^docker0'},\n",
       "   'Docker': {'BaseImage': 'mcr.microsoft.com/azureml/base-gpu:0.1.4',\n",
       "    'Enabled': True,\n",
       "    'SharedVolumes': True,\n",
       "    'GpuSupport': True,\n",
       "    'Arguments': [],\n",
       "    'BaseImageRegistry': {'Address': None,\n",
       "     'Username': None,\n",
       "     'Password': None}},\n",
       "   'Spark': {'Repositories': ['https://mmlspark.azureedge.net/maven'],\n",
       "    'Packages': [{'Group': 'com.microsoft.ml.spark',\n",
       "      'Artifact': 'mmlspark_2.11',\n",
       "      'Version': '0.12'}],\n",
       "    'PrecachePackages': True}},\n",
       "  'History': {'OutputCollection': True},\n",
       "  'Spark': {'Configuration': {'spark.app.name': 'Azure ML Experiment',\n",
       "    'spark.yarn.maxAppAttempts': '1'}},\n",
       "  'BatchAi': {'NodeCount': 1},\n",
       "  'Tensorflow': {'WorkerCount': 1, 'ParameterServerCount': 1},\n",
       "  'Mpi': {'ProcessCountPerNode': 1},\n",
       "  'Hdi': {'YarnDeployMode': 2},\n",
       "  'ContainerInstance': {'Region': None, 'CpuCores': 1, 'MemoryGb': 4},\n",
       "  'ExposedPorts': None,\n",
       "  'PrepareEnvironment': None},\n",
       " 'logFiles': {'azureml-logs/60_control_log.txt': 'https://jkamlstoragekhsgpljj.blob.core.windows.net/azureml/ExperimentRun/aerial-classifier-dataprep_1540157141770/azureml-logs/60_control_log.txt?sv=2017-04-17&sr=b&sig=KhkQfdVO4pI9sOBouIWzQ6R3uoLcBw39IJR0alM7dwI%3D&st=2018-10-21T21%3A17%3A30Z&se=2018-10-22T05%3A27%3A30Z&sp=r',\n",
       "  'azureml-logs/80_driver_log.txt': 'https://jkamlstoragekhsgpljj.blob.core.windows.net/azureml/ExperimentRun/aerial-classifier-dataprep_1540157141770/azureml-logs/80_driver_log.txt?sv=2017-04-17&sr=b&sig=Z1obUa7YfrAIJwUNueQfJR3cXOWaajkaBlQsxCj70WE%3D&st=2018-10-21T21%3A17%3A30Z&se=2018-10-22T05%3A27%3A30Z&sp=r',\n",
       "  'azureml-logs/azureml.log': 'https://jkamlstoragekhsgpljj.blob.core.windows.net/azureml/ExperimentRun/aerial-classifier-dataprep_1540157141770/azureml-logs/azureml.log?sv=2017-04-17&sr=b&sig=JY%2B3HPfJ9LnzQA9LkQ8niPx9DfYRFfO%2F3N0MV0p3bm4%3D&st=2018-10-21T21%3A17%3A30Z&se=2018-10-22T05%3A27%3A30Z&sp=r'}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Step\n",
    "The run has completed. The bottleneck files have been copied to the workspace's default datastore. You are ready to move to the next part of the lab in which you are going to train a small fully connected neural network using the bottleneck features.\n",
    "\n",
    "To proceed start the `02-train.ipynb` notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
