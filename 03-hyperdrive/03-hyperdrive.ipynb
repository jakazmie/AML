{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Hyper-parameter tuning\n",
    "In this section of the lab we fine tune hyper-parameters of our image classifier using Azure ML feature calle `hyperdrive`.\n",
    "\n",
    "\n",
    "![Transfer Learning](../images/TLArch.png)\n",
    "\n",
    "\n",
    "We will run training jobs in parallel on Azure Batch AI GPU cluster. After the model is fine tuned, the best version will be registered in AML Model Registry.\n",
    "\n",
    "![AML Arch](../images/amlarch.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training script\n",
    "\n",
    "We are using the same script as in the previous step.\n",
    "\n",
    "#### Create a folder to hold the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder = './script'\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Jupyter `%%writefile` magic to write the script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_folder/train.py\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "from azureml.core import Run\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import h5py\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "\n",
    "# Training regime\n",
    "def train_evaluate(run):\n",
    "   \n",
    "    print(\"Loading bottleneck features\")\n",
    "    train_file_name = os.path.join(args.data_folder, args.training_file_name)\n",
    "    valid_file_name = os.path.join(args.data_folder, args.validation_file_name)\n",
    "    \n",
    "    # Load bottleneck training features and labels\n",
    "    with h5py.File(train_file_name, \"r\") as hfile:\n",
    "        train_features = np.array(hfile.get('features'))\n",
    "        train_labels = np.array(hfile.get('labels'))\n",
    "        \n",
    "        \n",
    "    # Load bottleneck validation features and labels\n",
    "    with h5py.File(valid_file_name, \"r\") as hfile:\n",
    "        valid_features = np.array(hfile.get('features'))\n",
    "        valid_labels = np.array(hfile.get('labels'))\n",
    "        \n",
    "    # Conver one-hot labels to integers\n",
    "    y_train = np.argmax(train_labels, axis=1)\n",
    "    y_valid = np.argmax(valid_labels, axis=1)\n",
    "    \n",
    "    # Train logistics regresssion model\n",
    "    print(\"Starting training on\")\n",
    "    print(\"  Features:\", train_features.shape)\n",
    "    print(\"  Labels:\", y_train.shape)\n",
    "    clf = LogisticRegression(\n",
    "        C=1.0/args.reg, \n",
    "        multi_class='multinomial',\n",
    "        solver='lbfgs',\n",
    "        random_state=42)\n",
    "    clf.fit(train_features, y_train)\n",
    "    \n",
    "    \n",
    "    # Validate\n",
    "    print(\"Starting validation\")\n",
    "    y_hat = clf.predict(valid_features)\n",
    "    \n",
    "    # Calculate accuracy \n",
    "    acc = np.average(y_hat == y_valid)\n",
    "    print('Accuracy is:', acc)\n",
    "    \n",
    "    # Log to AML Experiment\n",
    "    run.log('regularization_rate', np.float(args.reg))\n",
    "    run.log('validation_acc', np.float(acc))\n",
    "          \n",
    "    # Save the trained model to outp'uts which is a standard folder expected by AML\n",
    "    model_file = 'aerial_sklearn.pkl'\n",
    "    model_file = os.path.join('outputs', model_file)\n",
    "    print(\"Saving the model to: \", model_file)\n",
    "    os.makedirs('outputs', exist_ok=True)\n",
    "    joblib.dump(value=clf, filename=model_file)\n",
    "    \n",
    "\n",
    "  \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(\"Training, evaluation worklfow\")\n",
    "\n",
    "    ### Model parameters\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--data-folder',\n",
    "        type=str,\n",
    "        default = './bottleneck',\n",
    "        help='Folder with bottleneck features and labels')\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--training-file-name',\n",
    "        type=str,\n",
    "        default = 'aerial_bottleneck_train.h5',\n",
    "        help='Training file name')\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--validation-file-name',\n",
    "        type=str,\n",
    "        default = 'aerial_bottleneck_valid.h5',\n",
    "        help='Validation file name')\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--regularization', \n",
    "        type=float, dest='reg', \n",
    "        default=0.01, \n",
    "        help='regularization rate')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # get hold of the current run\n",
    "    run = Run.get_submitted_run()\n",
    "    train_evaluate(run)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to AML workspace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create remote compute cluster\n",
    "\n",
    "We will use Azure Batch AI GPU cluster to run  hyper parameter tuning.\n",
    "\n",
    "The cluster is set up for autoscaling. It will start with a single node and can scale to up to 4 nodes. The nodes are NC6 VMs with Tesla K80 GPU.\n",
    "\n",
    "**Creation of the cluster takes approximately 5 minutes.** If the cluster is already in the workspace this code uses it and skips the creation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, BatchAiCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "batchai_cluster_name = ws.name + 'cpucls'\n",
    "vim_size = 'Standard_D4_v2'\n",
    "\n",
    "try:\n",
    "    # look for the existing cluster by name\n",
    "    compute_target = ComputeTarget(workspace=ws, name=batchai_cluster_name)\n",
    "    if type(compute_target) is BatchAiCompute:\n",
    "        print('found compute target {}, just use it.'.format(batchai_cluster_name))\n",
    "    else:\n",
    "        print('{} exists but it is not a Batch AI cluster. Please choose a different name.'.format(batchai_cluster_name))\n",
    "except ComputeTargetException:\n",
    "    print('creating a new compute target...')\n",
    "    compute_config = BatchAiCompute.provisioning_configuration(vm_size=vim_size, # GPU-based VM\n",
    "                                                                #vm_priority='lowpriority', # optional\n",
    "                                                                autoscale_enabled=True,\n",
    "                                                                cluster_min_nodes=1, \n",
    "                                                                cluster_max_nodes=4)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, batchai_cluster_name, compute_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it uses the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "    # Use the 'status' property to get a detailed status for the current cluster. \n",
    "    print(compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure datastore\n",
    "\n",
    "The bottleneck files have been uploaded to the workspace's default datastore during the previous step. We will mount the store on the nodes of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "\n",
    "ds = ws.get_default_datastore()\n",
    "print(\"Using the default datastore for training data: \")\n",
    "print(ds.name, ds.datastore_type, ds.account_name, ds.container_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "\n",
    "Although there are multiple hyper-parameters controling logistic regression, for the purpose of the lab, we will focus on one: regularization.\n",
    "\n",
    "So far we have executed a single training run with one value of regularizaton: 0.8. \n",
    "\n",
    "Now, we are going to use AML feature called *hyperdrive* to launch multiple runs on multiple cluster nodes using different values for regularization.\n",
    "\n",
    "\n",
    "First, let's define the hyperparameter space using grid sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import *\n",
    "\n",
    "ps = GridParameterSampling(\n",
    "    {\n",
    "        '--regularization': choice(0.01, 0.05, 0.1, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a new estimator without the above parameters since they will be passed in later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "script_params = {\n",
    "    '--data-folder': ds.path('bottleneck').as_download(),\n",
    "    '--training-file-name': 'aerial_bottleneck_train_vgg16.h5',\n",
    "    '--validation-file-name': 'aerial_bottleneck_valid_vgg16.h5'\n",
    "}\n",
    "\n",
    "pip_packages = ['h5py','pillow','scikit-learn']\n",
    "\n",
    "est = Estimator(source_directory=script_folder,\n",
    "                script_params=script_params,\n",
    "                compute_target=compute_target,\n",
    "                entry_script='train.py',\n",
    "                pip_packages=pip_packages\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hyperdrive* supports early termination policies to limit exploration of hyperparameter combinations that don't show promise of helping reach the target metric. This is feature is especially useful when traversing large hyperparameter spaces. Since we are going to run a small number of jobs we will not apply early termination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = NoTerminationPolicy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to configure a run configuration object, and specify the primary metric as *validation_acc* that's recorded in our training runs. If you go back to visit the training script, you will notice that this value is being logged after every run. We also want to tell the service that we are looking to maximizing this value. We also set the number of total runs to 20, and maximal concurrent job to 4, which is the same as the number of nodes in our computer cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htc = HyperDriveRunConfig(estimator=est, \n",
    "                          hyperparameter_sampling=ps,\n",
    "                          policy=policy,\n",
    "                          primary_metric_name='validation_acc', \n",
    "                          primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n",
    "                          max_total_runs=10,\n",
    "                          max_concurrent_runs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new experiment to capture `hyperdrive` runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'aerial-sklearn-hyperdrive'\n",
    "\n",
    "from azureml.core import Experiment\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's launch the hyperparameter tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = {\"RunName\": \"Hyperdrive-sklearn\"}\n",
    "\n",
    "hdr = exp.submit(config=htc, tags=tags)\n",
    "hdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.widgets import RunDetails\n",
    "RunDetails(hdr).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdr.wait_for_completion(show_output=True) # specify True for a verbose log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find and register best model\n",
    "When all the jobs finish, we can find out the one that has the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = hdr.get_best_run_by_primary_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run_metrics = best_run.get_metrics()\n",
    "parameter_values = best_run.get_details()['runDefinition']['Arguments']\n",
    "\n",
    "print('Best Run Id: ', best_run.id)\n",
    "print('\\n Validation Accuracy:', best_run_metrics['validation_acc'])\n",
    "print('\\n Regularization:',parameter_values[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the output of the best run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_run.get_file_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register model\n",
    "The last step in the training script wrote the file `aerial_classifier.hd5` in the `outputs` directory. As noted before, `outputs` is a special directory in that all content in this  directory is automatically uploaded to your workspace.  This content appears in the run record in the experiment under your workspace. \n",
    "\n",
    "You can register the model so that it can be later queried, examined and deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_run.register_model(model_name='aerial_sklearn', model_path='outputs/aerial_sklearn.pkl')\n",
    "print(model.name, model.id, model.version, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Step\n",
    "\n",
    "The model is now ready for deployment\n",
    "\n",
    "Proceed to `04-deploy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up resources\n",
    "Before you move to the next step, delete the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_target.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
